{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pkg_resources\ndef placeholder(x):\n    raise pkg_resources.DistributionNotFound\npkg_resources.get_distribution = placeholder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/fastaudio/fastaudio.git --quiet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.vision.all import *\nfrom fastaudio.core.all import *\nfrom fastaudio.augment.all import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/rfcx-species-audio-detection/train_tp.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.species_id = df.species_id.astype('str')\n\n# df = df.groupby(['recording_id'])['species_id'].apply(','.join).reset_index()\n\ndf = df.groupby(['recording_id'])['species_id'].apply(lambda x:','.join(list(set(x)))).reset_index()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.species_id.value_counts(ascending=True)[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path('../input/rfcx-species-audio-detection/train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fnames = get_files(path,extensions=audio_extensions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DBMelSpec = SpectrogramTransformer(mel=True, to_db=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aud2spec = DBMelSpec(f_min=90,f_max=14000,sample_rate=48000,\n                     normalized=True,n_fft=20*128,hop_length=215)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_tfms = [aud2spec]\n\nbatch_tfms = [Normalize.from_stats(*imagenet_stats)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dblock = DataBlock((AudioBlock,MultiCategoryBlock),\n                   get_x=ColReader('recording_id',pref=f'{path}/',suff='.flac'),\n                   get_y=ColReader('species_id',label_delim=','),\n                   splitter=RandomSplitter(),\n                   item_tfms=item_tfms,\n                   batch_tfms=batch_tfms\n                  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls = dblock.dataloaders(df,bs=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.show_batch(figsize=(15,7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _accumulate(self, learn):\n    #pred = learn.pred.argmax(dim=self.dim_argmax) if self.dim_argmax else learn.pred\n    m = nn.Sigmoid()\n    pred = learn.pred\n    pred = torch.round(m(pred))\n    targ = learn.y\n    pred,targ = to_detach(pred),to_detach(targ)\n    self.preds.append(pred)\n    self.targs.append(targ)\n\nAccumMetric.accumulate = _accumulate\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# taken from 'https://www.kaggle.com/c/rfcx-species-audio-detection/discussion/198418'\n\ndef LWLRAP(preds, labels):\n    # Ranks of the predictions\n    ranked_classes = torch.argsort(preds, dim=-1, descending=True)\n    # i, j corresponds to rank of prediction in row i\n    class_ranks = torch.zeros_like(ranked_classes)\n    for i in range(ranked_classes.size(0)):\n        for j in range(ranked_classes.size(1)):\n            class_ranks[i, ranked_classes[i][j]] = j + 1\n    # Mask out to only use the ranks of relevant GT labels\n    ground_truth_ranks = class_ranks * labels + (1e6) * (1 - labels)\n    # All the GT ranks are in front now\n    sorted_ground_truth_ranks, _ = torch.sort(ground_truth_ranks, dim=-1, descending=False)\n    # Number of GT labels per instance\n    num_labels = labels.sum(-1)\n    pos_matrix = torch.tensor(np.array([i+1 for i in range(labels.size(-1))])).unsqueeze(0)\n    score_matrix = pos_matrix / sorted_ground_truth_ranks\n    score_mask_matrix, _ = torch.sort(labels, dim=-1, descending=True)\n    scores = score_matrix * score_mask_matrix\n    score = scores.sum() / labels.sum()\n    return score.item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(dls,resnet50,\n                    n_in=1,\n                    metrics=[accuracy_multi,AccumMetric(LWLRAP)],model_dir='.').to_fp16()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fine_tune(10,freeze_epochs=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_subm = pd.read_csv('../input/rfcx-species-audio-detection/sample_submission.csv')\ndf_subm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_copy = df_subm.copy()\nsub_copy.recording_id=df_subm.recording_id.apply(lambda x: f'../test/{x}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"test_dl = dls.test_dl(sub_copy)\npreds= learn.get_preds(dl = test_dl)\n\ndf_subm.iloc[:,1:] = preds[0]\ndf_subm.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}